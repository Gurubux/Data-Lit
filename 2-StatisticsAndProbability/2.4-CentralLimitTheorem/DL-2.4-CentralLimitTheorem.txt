---------------------
CENTRAL LIMIT THEOREM
---------------------
CLT says,
	If we plot the frequency of the means of many sample with n =X, we do get a normal distribution.
	It a very good approximation for the sum or the means of a lot of processes.


As you increase your sample size, as you increase your n, and as you take a lot of sample means(or sum), you're going to have a frequency plot(Of the mean/sum) that looks very, very close to a normal distribution.
Example:
	For n=10
	Take a sample say 
		x₁ = [1,2,4,6,8,9,7,8,9,1,1] μₓ = 56/10 = 5.6
		x₂ = [1,3,5,5,9,9,7,8,8,1,1] μₓ = 61/10 = 6.1
		x₃ = [1,1,3,6,8,9,8,8,9,1,2] μₓ = 55/10 = 5.5 
		.
		.
		.
		x₁₀₀₀₀

		Plot the frquenncy of the mean(number of times each mean occured)
		It will get close to a normal distribution.

		As Sample counts approach to infinity we approach to closer normal deviation

Because in life, there's all sorts of processes out there, proteins bump crazy things, humans interacting in weird ways. And you don't know the probability distribution functions for any of those things. But what the central limit theorem tells us is if we add a bunch of those actions together, assuming that they all have the same distribution, or if we were to take the mean of all of those actions together, and if we were to plot the frequency of those means, we do get a normal distribution.

If you decide to take high sample size (s) per observation, your normal distribution narrows.

//////////////////////////////////
sample size (n) is the number of data points we get from the distribution.
observations (k) is the number of observations we made. Each observation would take n data points and calculate a value out of it (such as the mean).

The Central Limit Theorem states that with a sufficiently high number of observations (k), we get a normal distribution.

The Law of Large Numbers states that with sufficiently high sample size (n), our sample value (such as sample mean) is pretty close to the actual population value (such as population mean). Thus, the Law of Large Numbers is about one observation.

They are different, but are definitely related.

For example, I only talked about number of observations (k) above. No matter what your sample size (s) is you will get a normal distribution. However, did you know that the width of the normal distribution changes based on your sample size (s)? If you decide to take high sample size (s) per observation, your normal distribution narrows.

Since the normal distribution is more narrow, what is the chance that 1 observation made would give sample value close to population value? It would be more than when the normal distribution is wide. In conclusion, the Central Limit Theorem describes the distribution of sample values, while the Law of Large Numbers describes how likely one observation is close to the population value.
//////////////////////////////

The process of plotting is called 'SAMPLING DISTRIBUTION OF THE SAMPLE MEAN'
Reps,
Mean,
Median,
SD,
Skew,
Kurtosis

Skew 	 --> 0 Skew = Nice and symmetric Normal distribution(Perfect ND)
		 	 + Skew = Long tail to the right
		  	 - Skew = Long tail to the left
https://raw.githubusercontent.com/Gurubux/Data-Lit/master/2-StatisticsAndProbability/2.4-CentralLimitTheorem/SamplingDistribution_Skew.PNG

Kurtosis --> 0 Kurtosis = Nice and symmetric Normal distribution(Perfect ND)
		 	 + Kurtosis = Fatter tails, pointy peak
		  	 - Kurtosis = Smaller tails, smoother peak
https://raw.githubusercontent.com/Gurubux/Data-Lit/master/2-StatisticsAndProbability/2.4-CentralLimitTheorem/SamplingDistribution_Kurtosis.PNG

import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import math
# Obtaining Population ages from 18 with mean 35 and 18 with mean 10
np.random.seed(10)
population_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000)
population_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000)
population_ages = np.concatenate((population_ages1, population_ages2))
# Plot the population Histogram
pd.DataFrame(population_ages).hist(bins=58,
                                  range=(17.5,75.5),
                                  figsize=(9,9))

# Obtaining Sample ages size 500
np.random.seed(6)
sample_ages = np.random.choice(a= population_ages,
                               size=500)
# Plot the Sample Histogram
pd.DataFrame(sample_ages).hist(bins=58,
                                  range=(17.5,75.5),
                                  figsize=(9,9))


"Both the sample and population has roughly the same shape as the underlying population."

"**IMP** So we apply Central Limit theorem if Sample and Poplation do not have Normal distribution"
np.random.seed(10)
point_estimates = []         # Make empty list to hold point estimates

for x in range(200):         # Generate 200 samples
    sample = np.random.choice(a= population_ages, size=500)
    point_estimates.append( sample.mean() )
    
pd.DataFrame(point_estimates).plot(kind="density",  # Plot sample mean density
                                   figsize=(9,9),
                                   xlim=(41,45))   

print(print( stats.skew(population_ages) )) # -0.1200
print(print( stats.skew(sample_ages) )) # -0.05622
print(print( stats.skew(point_estimates) )) # 0.16159

print(population_ages.mean()) # 43.002372
print(sample_ages.mean())  # 42.388
print(np.array(point_estimates).mean())  # 43.08678

# Check difference between means
print(population_ages.mean() - sample_ages.mean() )  # 0.61437200000000303
print(population_ages.mean() - np.array(point_estimates).mean()) # -0.08440799999999626
"the mean of the sampling distribution approaches the true population mean"


#The more samples we take, the better our estimate of the population parameter is likely to be.