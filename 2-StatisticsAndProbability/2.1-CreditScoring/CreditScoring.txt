Statistics is collection of procedures and principles for gaining information in order to make decisions when faced with uncertainty.
Concepts to Learn in statistics:
	1. Statistical Features
	2. Probability Distributions
	3. Bayesian Statistics

Data scientist is better at programming than any statistician and better at stats than any programmer.
Examples :
	-Identify risk factors for a disease
	-Customize Spam detection
	-Establish the relationship between variables.

Exercise : Lending Club Loan History to check if Credit Scores are correct and predict more optimized Credit Scores using ML
	 Goal is to analyze the structure of the data using DS techniques and then further apply techniques to optimize

1. Statistical Features
	Bias,Variance, Mean, Median, weighted Avg, Std Deviation, Quartiles, 5-Number Summary,Percentiles, Z-score Solver, Central Limit Theorem : 
	Box-Plot

2. Probability Distributions: 
	Probability : Percent chance of an event happening

3. Bayesian Statistics
	Frequency statistics

4,5,6.. Up Sampling, Down Sampling, Dimensionality Reduction and more..

SUMMARY
Statistical Features like Bias, Variance and many others help us Explore A Dataset to Gain valuable insights.
Probability Distributions define the percent chance that some event will occur and we can use them to understand the spread of Data.
Bayesian Statistics expresses probability as a degree of belief in an event which can change as new information is gathered rather than a fixed value based frequency.

Coding : Loan Default Prediction
-------------
DATA CLEANING
-------------
MERGING DATA
	dataset = pd.concat([df2012_13, df2014])

REMOVING COLUMNS
	empty_cols = [i for i in range(45,72)]
	dataset = dataset.drop(dataset.columns[empty_cols],axis=1)

	dataset=Dataset_withBoolTarget.dropna(thresh = 340000,axis=1) #340000 is minimum number of non-NA values

	del_col_names = ["delinq_2yrs",  "last_pymnt_d", "chargeoff_within_12_mths","delinq_amnt","emp_title", "term", "emp_title",	"pymnt_plan","purpose","title", "zip_code", "verification_status", "dti","earliest_cr_line", "initial_list_status", "out_prncp", "pymnt_plan", "num_tl_90g_dpd_24m", "num_tl_30dpd", "num_tl_120dpd_2m", "num_accts_ever_120_pd", "delinq_amnt",  "chargeoff_within_12_mths", "total_rec_late_fee", "out_prncp_inv", "issue_d"] #deleting some more columns
	dataset = dataset.drop(labels = del_col_names, axis = 1) 

CONVERTING TARGET VARIABLE TO BOOLEAN
	di = {"Fully Paid":0, "Charged Off":1}   
	Dataset_withBoolTarget= data_with_loanstatus_sliced.replace({"loan_status": di})

-------------------
DATA TRANSFORMATION
-------------------
#grade - Borrowers grade given basing on his/her past history - encoded to numerical values.
Final_data[`grade`] = Final_data[`grade`].map({`A`:7,`B`:6,`C`:5,`D`:4,`E`:3,`F`:2,`G`:1})

#home_ownership - this is feature in the dataset which had to be encoded to numerical values. 
Final_data["home_ownership"] = Final_data["home_ownership"].map({"MORTGAGE":6,"RENT":5,"OWN":4,"OTHER":3,"NONE":2,"ANY":1})

#Emp_Length - this feature was not formatted properly. It has some values which was in the format like
#"10+years","5years"...etc. we changed them to numerical values in the below cell.
Final_data["emp_length"] = Final_data["emp_length"].replace({`years`:``,`year`:``,` `:``,`<`:``,`\+`:``,`n/a`:`0`}, regex = True)
Final_data["emp_length"] = Final_data["emp_length"].apply(lambda x:int(x))

In Code error Fix:
https://github.com/llSourcell/LoanDefault-Prediction/blob/master/Loan_Default_Prediction_Final.ipynb
–> Final_data[“emp_length”] = Final_data[“emp_length”].apply(lambda x:int(x))
Error : ValueError: cannot convert float NaN to integer
–> Final_data[“emp_length”] = Final_data[“emp_length”].apply(lambda x:pd.to_numeric(x, downcast=’integer’))


------------------------------------------
FILLING MISSING VALUES AND FEATURE SCALING
------------------------------------------
> We have some important features which have some missing values. We filled those missing those values with the mean of the column. 
Final_data.fillna(Final_data.mean(),inplace = True)

> We scaled the features all the features here using standard scaler. 
scl = preprocessing.StandardScaler() #instance of preprocessing
fields = Final_data.columns.values[:-1]
data_clean = pd.DataFrame(scl.fit_transform(Final_data[fields]), columns = fields)
data_clean['loan_status'] = Final_data['loan_status']
data_clean['loan_status'].value_counts()

> We sampled our dataset here after infering from the learning curve plotted.
loanstatus_0 = data_clean[data_clean["loan_status"]==0]
loanstatus_1 = data_clean[data_clean["loan_status"]==1]
subset_of_loanstatus_0 = loanstatus_0.sample(n=5500)
subset_of_loanstatus_1 = loanstatus_1.sample(n=5500)
data_clean = pd.concat([subset_of_loanstatus_1, subset_of_loanstatus_0])
data_clean = data_clean.sample(frac=1).reset_index(drop=True)
print("Current shape of dataset :",data_clean.shape)
data_clean.head()

> Below are correlation values between the features finally selected.
data_clean.corr()
------------------------------------------
LEARNING CURVE
------------------------------------------
------------------------------------------
ROC CURVE PLOT FUNCTION
------------------------------------------
------------------------------------------
CONFUSION MATRIX VIZ FUNCTION
------------------------------------------
------------------------------------------
FEATURE SELECTION USING RFE (RECURSIVE FEATURE ELIMINATION)

------------------------------------------
------------------------------------------
PCA (PRINCIPAL COMPONENT ANALYSIS)
------------------------------------------
https://github.com/llSourcell/LoanDefault-Prediction/blob/master/Loan_Default_Prediction_Final.ipynb

------------------------------------------------------------------------------------------------------------------------------
MEAN, MEDIAN, MODE, GEOMETRIC MEAN, HARMONIC MEAN, VARIANCE, STANDARD DEVIATION, COVARIANCE, CORELATION ¹²³⁴⁵⁶⁷⁸⁹⁰ *******
------------------------------------------
MEAN   	μ	: ( Σ xᵢ ) / n
MEDIAN 		: Middle of a data set.
MODE   		: Value that occurs the most often.
GEAOMETRIC 	: n√(a1 × a2 × ... × an)
	MEAN 		
HARMONIC	: n / Σ (1/xᵢ)
	MEAN
VARIANCE σ²	: 1/n Σ (xᵢ - μ)²

STANDARD σ	: √σ²
   DEVIATION

NORMAL DISTRIBUTION - https://www.mathsisfun.com/data/standard-normal-distribution.html
Data can be "distributed" (spread out) in different ways. But there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a "Normal Distribution"
	      _
	     /|\ 
	    /|||\ 
	   /|||||\ 
	  /|||||||\ 
	_/|||||||||\_
The Normal Distribution has:
	- mean = median = mode
	- symmetry about the center
	- 50% of values less than the mean and 50% greater than the mean
	- likely to be within 1 standard deviation (68 out of 100 should be)
	- very likely to be within 2 standard deviations (95 out of 100 should be)
	- almost certainly within 3 standard deviations (997 out of 1000 should be)

Z- SCORE : The number of standard deviations from the mean is also called the "Standard Score", "sigma" or "z-score". 
    ______________  
   /    x - μ	 /
  / z = ______  / 
 /	      σ	   / 
/_____________/

STANDARD NORMAL DISTRIBUTION Table - https://www.mathsisfun.com/data/standard-normal-distribution-table.html


z is the "z-score" (Standard Score)
x is the value to be standardized
μ is the mean
σ is the standard deviation

# A Practical Example: Your company packages sugar in 1 kg bags.
	When you weigh a sample of bags you get these results:
		1007g, 1032g, 1002g, 983g, 1004g, ... (a hundred measurements)
		Mean = 1010g
		Standard Deviation = 20g
==> 31% of the bags are less than 1000g,  which is cheating the customer!
It is a random thing, so we can`t stop bags having less than 1000g, but we can try to reduce it a lot.

Let`s adjust the machine so that 1000g is:
	at −3 standard deviations:
		From the big bell curve above we see that 0.1% are less. But maybe that is too small.
	at −2.5 standard deviations:
		Below 3 is 0.1% and between 3 and 2.5 standard deviations is 0.5%, together that is 0.1% + 0.5% = 0.6% (a good choice I think)

So let us adjust the machine to have 1000g at −2.5 standard deviations from the mean.
Now, we can adjust it to:
	increase the amount of sugar in each bag (which changes the mean), or
	make it more accurate (which reduces the standard deviation)

Let us try both.

ADJUST THE MEAN AMOUNT IN EACH BAG
The standard deviation is 20g, and we need 2.5 of them:
    ______________  				
   /     x - μ	 /
  / z = ______  /  = -0.5
 /	      σ	   / 		
/_____________/
    __________________  
   /       1000 - μ	 /
  / -2.5 = ______   / 
 /	         20	   / 
/_________________/

(2.5 × 20g = 50g)
μ = 1000 + 50 = 1050

So the machine should average 1050g, like this: https://www.mathsisfun.com/data/images/normal-distribution-ex2.gif


Or
ADJUST THE ACCURACY OF THE MACHINE
We can keep the same mean (of 1010g), but then we need 2.5 standard deviations to be equal to 10g:
    __________________  
   /      1000 - 1010/
  / -2.5 = ______   / 
 /	         σ     / 
/_________________/

σ = 1000 - 1010 / -2.5
  = 10g / 2.5 = 4g

So the standard deviation should be 4g, like this: <img src=`https://www.mathsisfun.com/data/images/normal-distribution-ex3.gif`/>

Or perhaps we could have some combination of better accuracy and slightly larger average size, I will leave that up to you!


CORELATION
A correlation is a statistical measure of the relationship between two variables. The measure is best used in variables that demonstrate a linear relationship between each other. The fit of the data can be visually represented in a scatterplot.

The correlation COEFFICIENT is a value that indicates the strength of the relationship. The coefficient can take any values from -1 to 1. The interpretations of the values are:
	-1: Perfect negative correlation. The variables tend to move in opposite directions (i.e., when one variable increases, the other variable decreases).
	0 : No correlation. The variables do not have a relationship with each other.
	1 : Perfect positive correlation. The variables tend to move in the same direction (i.e., when one variable increases, the other variable also increases).

If two variables are correlated, it does not imply that one variable causes the changes in another variable. Correlation only assesses relationships between variables, and there may be different factors that lead to the relationships. CAUSATION may be a reason for the correlation, but it is not the only possible explanation.
CORELATION FORMULAE
 	__________________________________________  
   /         	Σ (xᵢ - x̅) (yᵢ - ȳ)			 /
  / r(xy) = ______________________________  / 
 /	         √(Σ (xᵢ - x̅)² Σ(yᵢ - ȳ)²)     / 
/_________________________________________/
	rxy – the correlation coefficient of the linear relationship between the variables x and y
	xᵢ – the values of the x-variable in a sample
	x̅ – the mean of the values of the x-variable
	yᵢ – the values of the y-variable in a sample
	ȳ – the mean of the values of the y-variable



COVARIANCE
Covariance is a measure of the relationship between two random variables. The metric evaluates how much – to what extent – the variables change together. In other words, it is essentially a measure of the variance between two variables (note that the variance of one variable equals the variance of the other variable). However, the metric does not assess the dependency between variables.
Unlike the correlation coefficient, covariance is measured in units. The units are computed by multiplyᵢng the units of the two variables. The variance can take any positive or negative values. The values are interpreted as follows:
	Positive covariance: Indicates that two variables tend to move in the same direction.
	Negative covariance: Reveals that two variables tend to move in inverse directions.

 	__________________________________________    __________________________________________
   /         		Σ (xᵢ - x̅) (yᵢ - ȳ)		 /   /         		Σ (xᵢ - x̅) (yᵢ - ȳ)		   /
  / Cov(x,y) = ___________________________  /   / Cov(x,y) = ___________________________  /   
 /	        			n 				   /   /	        			n-1			     / 
/_________________________________________/   /_________________________________________/
		Population COVARIANCE 								Sample COVARIANCE


COVARIANCE VS. CORRELATION
Covariance and correlation both primarily assess the relationship between variables. The closest analogy to the relationship between them is the relationship between the variance and standard deviation.

Covariance measures the total variation of two random variables from their expected values. Using covariance, we can only gauge the direction of the relationship (whether the variables tend to move in tandem or show an inverse relationship). However, it does not indicate the strength of the relationship, nor the dependency between the variables.

On the other hand, correlation measures the strength of the relationship between variables. Correlation is the scaled measure of covariance. It is dimensionless. In other words, the correlation coefficient is always a pure value and not measured in any units.

The relationship between the two concepts can be expressed using the formula below:
		_____________________________
	   /	  		Cov(x,y)		/
	  /	p(X,Y) = __________        / 
	 /				σx σy         /
	/____________________________/
****************MEAN, MEDIAN, MODE, GEOMETRIC MEAN, HARMONIC MEAN, VARIANCE, COVARIANCE, CORELATION ¹²³⁴⁵⁶⁷⁸⁹⁰****************


####################FOOTER################################
https://github.com/llSourcell/LoanDefault-Prediction
https://www.lendingclub.com/info/download-data.action
####################FOOTER################################