"https://github.com/Gurubux/Data-Lit/blob/master/2-StatisticsAndProbability/2.1-CreditScoring/Loan_Default_Prediction.ipynb"
Statistics is collection of procedures and principles for gaining information in order to make decisions when faced with uncertainty.
Concepts to Learn in statistics:
	1. Statistical Features
	2. Probability Distributions
	3. Bayesian Statistics

Data scientist is better at programming than any statistician and better at stats than any programmer.
Examples :
	-Identify risk factors for a disease
	-Customize Spam detection
	-Establish the relationship between variables.

Exercise : Lending Club Loan History to check if Credit Scores are correct and predict more optimized Credit Scores using ML
	 Goal is to analyze the structure of the data using DS techniques and then further apply techniques to optimize

1. Statistical Features
	Bias,Variance, Mean, Median, weighted Avg, Std Deviation, Quartiles, 5-Number Summary,Percentiles, Z-score Solver, Central Limit Theorem : 
	Box-Plot

2. Probability Distributions: 
	Probability : Percent chance of an event happening

3. Bayesian Statistics
	Frequency statistics

4,5,6.. Up Sampling, Down Sampling, Dimensionality Reduction and more..

SUMMARY
Statistical Features like Bias, Variance and many others help us Explore A Dataset to Gain valuable insights.
Probability Distributions define the percent chance that some event will occur and we can use them to understand the spread of Data.
Bayesian Statistics expresses probability as a degree of belief in an event which can change as new information is gathered rather than a fixed value based frequency.

Coding : Loan Default Prediction
-------------
DATA CLEANING
-------------
MERGING DATA
	dataset = pd.concat([df2012_13, df2014])

CONVERTING TARGET VARIABLE TO BOOLEAN
	di = {"Fully Paid":0, "Charged Off":1}   
	Dataset_withBoolTarget= data_with_loanstatus_sliced.replace({"loan_status": di})

REMOVING COLUMNS
	empty_cols = [i for i in range(45,72)]
	dataset = dataset.drop(dataset.columns[empty_cols],axis=1)

	dataset=Dataset_withBoolTarget.dropna(thresh = 340000,axis=1) #340000 is minimum number of non-NA values, Removing all columns having more than 340000 Nan values

	del_col_names = ["delinq_2yrs",  "last_pymnt_d", "chargeoff_within_12_mths","delinq_amnt","emp_title", "term", "emp_title",	"pymnt_plan","purpose","title", "zip_code", "verification_status", "dti","earliest_cr_line", "initial_list_status", "out_prncp", "pymnt_plan", "num_tl_90g_dpd_24m", "num_tl_30dpd", "num_tl_120dpd_2m", "num_accts_ever_120_pd", "delinq_amnt",  "chargeoff_within_12_mths", "total_rec_late_fee", "out_prncp_inv", "issue_d"] #deleting some more columns
	dataset = dataset.drop(labels = del_col_names, axis = 1) 


-------------------
DATA TRANSFORMATION
-------------------
#grade - Borrowers grade given basing on his/her past history - encoded to numerical values.
Final_data[`grade`] = Final_data[`grade`].map({`A`:7,`B`:6,`C`:5,`D`:4,`E`:3,`F`:2,`G`:1})

#home_ownership - this is feature in the dataset which had to be encoded to numerical values. 
Final_data["home_ownership"] = Final_data["home_ownership"].map({"MORTGAGE":6,"RENT":5,"OWN":4,"OTHER":3,"NONE":2,"ANY":1})

#Emp_Length - this feature was not formatted properly. It has some values which was in the format like
#"10+years","5years"...etc. we changed them to numerical values in the below cell.
Final_data["emp_length"] = Final_data["emp_length"].replace({`years`:``,`year`:``,` `:``,`<`:``,'\+':``,`n/a`:`0`}, regex = True)
Final_data["emp_length"] = Final_data["emp_length"].apply(lambda x:int(x))

In Code error Fix:
https://github.com/llSourcell/LoanDefault-Prediction/blob/master/Loan_Default_Prediction_Final.ipynb
–> Final_data[“emp_length”] = Final_data[“emp_length”].apply(lambda x:int(x))
Error : ValueError: cannot convert float NaN to integer
–> Final_data[“emp_length”] = Final_data[“emp_length”].apply(lambda x:pd.to_numeric(x, downcast=’integer’))


------------------------------------------
FILLING MISSING VALUES AND FEATURE SCALING
------------------------------------------
> We have some important features which have some missing values. We filled those missing those values with the mean of the column. 
Final_data.fillna(Final_data.mean(),inplace = True)

> We scaled all the features here using standard scaler. 
scl = preprocessing.StandardScaler() #instance of preprocessing
fields = Final_data.columns.values[:-1]
data_clean = pd.DataFrame(scl.fit_transform(Final_data[fields]), columns = fields)
data_clean['loan_status'] = Final_data['loan_status']
data_clean['loan_status'].value_counts() #counts of unique values.

> We sampled our dataset here after infering from the learning curve plotted.
loanstatus_0 = data_clean[data_clean["loan_status"]==0]
loanstatus_1 = data_clean[data_clean["loan_status"]==1]
subset_of_loanstatus_0 = loanstatus_0.sample(n=5500)
subset_of_loanstatus_1 = loanstatus_1.sample(n=5500)
data_clean = pd.concat([subset_of_loanstatus_1, subset_of_loanstatus_0])
data_clean = data_clean.sample(frac=1).reset_index(drop=True)
print("Current shape of dataset :",data_clean.shape)
data_clean.head()

------------------------------------------
CORRELATION COEFFICIENT PLOTTING
------------------------------------------
"https://github.com/Gurubux/ML-Analysis-Steps/blob/master/Correlation_Coefficient/Correlation_Coefficient.txt"

> Below are correlation values between the features finally selected.
corr = data_clean.corr()

## Type 1
def plot_Correlation_Coefficient_1(corr):
    sns.set_context(context='notebook')
    fig, ax = plt.subplots(figsize=(10,10)) 

    # Generate a mask for the upper triangle
    mask = np.zeros_like(corr, dtype=np.bool)
    mask[np.tril_indices_from(mask)] = True

    # Generate a custom diverging colormap
    cmap = sns.diverging_palette(220, 10, as_cmap=True)

    sns.heatmap(corr, cmap=cmap,linewidths=1, vmin=-1, vmax=1, square=True, cbar=True, center=0, ax=ax, mask=mask)
    plt.show()

## Type 2
def plot_Correlation_Coefficient_2(corr):
    sns.heatmap(corr, vmax=.8, square=True);
    plt.show()

------------------------------------------
CROSS-VALIDATION
------------------------------------------
"https://github.com/Gurubux/Udemy-ML/blob/master/Machine_Learning_A-Z/Part2-Regression/Cross-Validation/Cross_Validation.txt"
# Cross validation with 100 iterations to get smoother mean test and train score curves, each time with 20% data randomly selected as a validation set.
from sklearn.model_selection import ShuffleSplit
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
OR 
from sklearn.model_selection import cross_val_score
cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10)
accuracy = cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of SVM is: " , accuracy)

------------------------------------------
LEARNING CURVE
------------------------------------------
https://www.dataquest.io/blog/learning-curves-machine-learning/
https://github.com/Gurubux/StatQuest/blob/master/LearningCurve/LearningCurve.txt

plt = plot_learning_curve(estimator, title, X, y, ylim=(0.75, 0.90), cv=cv, n_jobs=4)

plot_learning_curve_2(model, electricity, features, target, train_sizes, 5)

------------------------------------------
ROC CURVE PLOT FUNCTION
------------------------------------------
"https://github.com/Gurubux/Udemy-ML/tree/master/Machine_Learning_A-Z/Part2-Regression/ROC_AUC"
def plotAUC(truth, pred, lab):
    fpr, tpr, _ = metrics.roc_curve(truth,pred)
    roc_auc = metrics.auc(fpr, tpr)
    lw = 2
    c = (np.random.rand(), np.random.rand(), np.random.rand())
    plt.plot(fpr, tpr, color= c,lw=lw, label= lab +'(AUC = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC curve') #Receiver Operating Characteristic 
    plt.legend(loc="lower right")


------------------------------------------
CONFUSION MATRIX VIZ FUNCTION
------------------------------------------
from sklearn.metrics import confusion_matrix
plot_confusion_matrix2(rfPredict, normalize=True)



@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
\LEFT TO STUDY - START
------------------------------------------------------------
FEATURE SELECTION USING RFE (RECURSIVE FEATURE ELIMINATION)
------------------------------------------------------------
from sklearn.feature_selection import RFE
# create the RFE model and select 3 attributes
clf_LR = linear_model.LogisticRegression(C=1e30)
clf_LR.fit(X_train,y_train)
rfe = RFE(clf_LR, 10)
rfe = rfe.fit(data_clean.iloc[:,:-1].values, data_clean.iloc[:,-1].values)
# summarize the selection of the attributes
print(rfe.support_)
print(rfe.ranking_)
# ['funded_amnt','emp_length','annual_inc','home_ownership','grade',"last_pymnt_amnt", "mort_acc", "pub_rec", 
# "int_rate", "open_acc","num_actv_rev_tl","mo_sin_rcnt_rev_tl_op","mo_sin_old_rev_tl_op","bc_util","bc_open_to_buy",
#"avg_cur_bal","acc_open_past_24mths",'loan_status']
------------------------------------------
PCA (PRINCIPAL COMPONENT ANALYSIS)
------------------------------------------
#https://github.com/llSourcell/LoanDefault-Prediction/blob/master/Loan_Default_Prediction_Final.ipynb
#PCA (Principal Component Analysis)
from sklearn.decomposition import PCA 
pca = PCA(n_components=10, whiten=True)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
explained_variance = pca.explained_variance_ratio_
print('Expected Variance is '+ str(explained_variance))

--------------------------------------------------------------------------
CLASSIFICATION REPORT - Precision,recall,F1score for all models
--------------------------------------------------------------------------
from sklearn.metrics import classification_report
print("RF",classification_report(y_test, rfPredict, target_names=None))
print("SVM",classification_report(y_test, predictions_svm, target_names=None))
print("LR",classification_report(y_test, LR_Predict_bin, target_names=None))
print("KNN",classification_report(y_test, knn_pred, target_names=None))
print("MLP",classification_report(y_test, predict_NN, target_names=None))

RF              precision    recall  f1-score   support

          0       0.84      0.79      0.82      1109
          1       0.80      0.85      0.82      1091

avg / total       0.82      0.82      0.82      2200

SVM              precision    recall  f1-score   support

          0       0.92      0.71      0.81      1109
          1       0.76      0.94      0.84      1091

avg / total       0.84      0.83      0.82      2200

LR              precision    recall  f1-score   support

          0       0.93      0.71      0.81      1109
          1       0.76      0.94      0.84      1091

avg / total       0.85      0.83      0.83      2200

KNN              precision    recall  f1-score   support

          0       0.83      0.70      0.76      1109
          1       0.74      0.85      0.79      1091

avg / total       0.78      0.77      0.77      2200

MLP              precision    recall  f1-score   support

          0       0.91      0.74      0.82      1109
          1       0.78      0.93      0.85      1091

avg / total       0.85      0.83      0.83      2200




------------------------------------------
MOD BOOTSTRAPPING
------------------------------------------
Behaviour of models with different sample sizes of dataset
def modBootstrapper(train, test, nruns, sampsize, model, c):
    target = 'loan_status'
    aucs_boot = []
    for i in range(nruns):
        train_samp = train.iloc[np.random.randint(0, len(train), size = sampsize)] #selecting random indexes for KFold
        if (model == "LR"):
            lr_i = linear_model.LogisticRegression(C = 1e30)
            lr_i.fit(train_samp.drop(target,1), train_samp[target]) #Logistic regression
            p = lr_i.predict_proba(test.drop(target,1))[:,1]
        elif (model == "SVM"):
            svm_i = svm.SVC(kernel='rbf', C = c) 
            svm_i.fit(train_samp.drop(target,1), train_samp[target])#SVM fitting and predicting if lr==0
            p = svm_i.decision_function(test.drop(target,1))
        elif (model == "RF"):
            RF_i = RandomForestClassifier(bootstrap=True,criterion = "gini")
            RF_i.fit(train_samp.drop(target,1), train_samp[target])
            p = RF_i.predict_proba(X_test)[:,1]
        elif (model == "KNN"):
            knn_i = KNeighborsClassifier(n_neighbors= 30) #taking the the best from the above cell and using it to find predictions
            knn_i.fit(train_samp.drop(target,1), train_samp[target])
            p = knn_i.predict_proba(X_test)[:,1]
            
        aucs_boot.append(metrics.roc_auc_score(test[target], p)) #calculating auc scores for each bag in bootstrapping
    
    return [np.mean(aucs_boot), np.sqrt(np.var(aucs_boot))] #mean, standard error = square root of variance



bs_train, bs_test = train_test_split(data_clean, test_size = 0.2, random_state=42) #just for bootstrapping
SampleSizes = [250,1000,1500,2000,2750,3750,4500,5200,6500,7000,8000,8500,9000,10000,11000] #various samples of Dataset
LR_means = []
Lr_stderr = []
svm_means = []
svm_stderr = []
RF_means = []
RF_stderr = []
KNN_means = []
KNN_stderr = []
for n in SampleSizes:
    mean, err = modBootstrapper(bs_train, bs_test, 20, n, "LR", 0.1)# collecting means and stderrs for LR model
    LR_means.append(mean)
    Lr_stderr.append(err)
    mean2, err2 = modBootstrapper(bs_train, bs_test, 20, n,"SVM", 0.1)# collecting means and stderrs for SVM model
    svm_means.append(mean2)
    svm_stderr.append(err2)
    mean3, err3 = modBootstrapper(bs_train, bs_test, 20, n,"RF", 0.1)# collecting means and stderrs for SVM model
    RF_means.append(mean3)
    RF_stderr.append(err3)
    mean4, err4 = modBootstrapper(bs_train, bs_test, 20, n,"KNN", 0.1)# collecting means and stderrs for SVM model
    KNN_means.append(mean4)
    KNN_stderr.append(err4)
    print(n)
>>>
250
1000
1500
2000
2750
3750
4500
5200
6500
7000
8000
8500
9000
10000
11000
------------------------------------------
LOGARITHMIC CURVE PLOTTING
------------------------------------------
plt.plot(np.log2(SampleSizes), LR_means, 'r', label = 'LR means')
plt.plot(np.log2(SampleSizes), LR_means + np.array(Lr_stderr), 'r+-', label = 'LR means + stderr')
plt.plot(np.log2(SampleSizes), LR_means - np.array(Lr_stderr), 'r--',  label = 'LR means + stderr')

plt.plot(np.log2(SampleSizes), svm_means, 'g', label = 'SVM means')
plt.plot(np.log2(SampleSizes), svm_means + np.array(svm_stderr), 'g+-', label = 'SVM means + stderr')
plt.plot(np.log2(SampleSizes), svm_means - np.array(svm_stderr), 'g--', label = 'SVM means - stderr')

plt.plot(np.log2(SampleSizes), RF_means, 'b', label = 'RF means')
plt.plot(np.log2(SampleSizes), RF_means + np.array(RF_stderr), 'b+-', label = 'RF means + stderr')
plt.plot(np.log2(SampleSizes), RF_means - np.array(RF_stderr), 'b--', label = 'RF means - stderr')

plt.plot(np.log2(SampleSizes), KNN_means, 'm', label = 'KNN means')
plt.plot(np.log2(SampleSizes), KNN_means + np.array(KNN_stderr), 'm+-', label = 'KNN means + stderr')
plt.plot(np.log2(SampleSizes), KNN_means - np.array(KNN_stderr), 'm--', label = 'KNN means - stderr')

plt.legend(bbox_to_anchor=(1.20, 0.5),loc = 10)
plt.xlabel('Log2(Sample Sizes)')
plt.ylabel('roc_auc_score')


------------------------------------------
BAGGING, MODEL EVALUATION, KFOLD, ENSEMBLE
------------------------------------------
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
model_rf = BaggingClassifier(base_estimator=randomForest, n_estimators=100, random_state=7)

model_lr = BaggingClassifier(base_estimator=clf_LR, random_state=7)

Ada_clf = AdaBoostClassifier(n_estimators=50)

results = model_selection.cross_val_score(model_rf, data_clean.iloc[:,:-1].values, data_clean.iloc[:,-1].values, cv=kfold)
print(results.mean()) #0.8298181818181819

results = model_selection.cross_val_score(model_lr, data_clean.iloc[:,:-1].values, data_clean.iloc[:,-1].values, cv=kfold)
print(results.mean()) #0.8193636363636363

scores = cross_val_score(Ada_clf, data_clean.iloc[:,:-1].values, data_clean.iloc[:,-1].values)
print(results.mean()) #0.8212723023101454



\LEFT TO STUDY - END
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@




"ADDITIONAL NOTES"
------------------------------------------------------------------------------------------------------------------------------
MEAN, MEDIAN, MODE, GEOMETRIC MEAN, HARMONIC MEAN, VARIANCE, STANDARD DEVIATION, Z-SCORE, COVARIANCE, CORRELATION ¹²³⁴⁵⁶⁷⁸⁹⁰ *******
------------------------------------------
MEAN   	μ	: ( Σ xᵢ ) / n
MEDIAN 		: Middle of a data set.
MODE   		: Value that occurs the most often.
GEAOMETRIC 	: n√(a1 × a2 × ... × an)
	MEAN 		
HARMONIC	: n / Σ (1/xᵢ)
	MEAN
VARIANCE σ²	: 1/n Σ (xᵢ - μ)²

STANDARD σ	: √σ²
   DEVIATION

NORMAL DISTRIBUTION - https://www.mathsisfun.com/data/standard-normal-distribution.html
Data can be "distributed" (spread out) in different ways. But there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a "Normal Distribution"
	      _
	     /|\ 
	    /|||\ 
	   /|||||\ 
	  /|||||||\ 
	_/|||||||||\_
The Normal Distribution has:
	- mean = median = mode
	- symmetry about the center
	- 50% of values less than the mean and 50% greater than the mean
	- likely to be within 1 standard deviation (68 out of 100 should be)
	- very likely to be within 2 standard deviations (95 out of 100 should be)
	- almost certainly within 3 standard deviations (997 out of 1000 should be)

Z- SCORE : The number of standard deviations from the mean is also called the "Standard Score", "sigma" or "z-score". 
    ______________  
   /    x - μ	 /
  / z = ______  / 
 /	      σ	   / 
/_____________/

STANDARD NORMAL DISTRIBUTION Table - https://www.mathsisfun.com/data/standard-normal-distribution-table.html


z is the "z-score" (Standard Score)
x is the value to be standardized
μ is the mean
σ is the standard deviation

# A Practical Example: Your company packages sugar in 1 kg bags.
	When you weigh a sample of bags you get these results:
		1007g, 1032g, 1002g, 983g, 1004g, ... (a hundred measurements)
		Mean = 1010g
		Standard Deviation = 20g
==> 31% of the bags are less than 1000g,  which is cheating the customer!
It is a random thing, so we can`t stop bags having less than 1000g, but we can try to reduce it a lot.

Let`s adjust the machine so that 1000g is:
	at −3 standard deviations:
		From the big bell curve above we see that 0.1% are less. But maybe that is too small.
	at −2.5 standard deviations:
		Below 3 is 0.1% and between 3 and 2.5 standard deviations is 0.5%, together that is 0.1% + 0.5% = 0.6% (a good choice I think)

So let us adjust the machine to have 1000g at −2.5 standard deviations from the mean.
Now, we can adjust it to:
	increase the amount of sugar in each bag (which changes the mean), or
	make it more accurate (which reduces the standard deviation)

Let us try both.

ADJUST THE MEAN AMOUNT IN EACH BAG
The standard deviation is 20g, and we need 2.5 of them:
    ______________  				
   /     x - μ	 /
  / z = ______  /  = -0.5
 /	      σ	   / 		
/_____________/
    __________________  
   /       1000 - μ	 /
  / -2.5 = ______   / 
 /	         20	   / 
/_________________/

(2.5 × 20g = 50g)
μ = 1000 + 50 = 1050

So the machine should average 1050g, like this: https://www.mathsisfun.com/data/images/normal-distribution-ex2.gif


Or
ADJUST THE ACCURACY OF THE MACHINE
We can keep the same mean (of 1010g), but then we need 2.5 standard deviations to be equal to 10g:
    __________________  
   /      1000 - 1010/
  / -2.5 = ______   / 
 /	         σ     / 
/_________________/

σ = 1000 - 1010 / -2.5
  = 10g / 2.5 = 4g

So the standard deviation should be 4g, like this: <img src=`https://www.mathsisfun.com/data/images/normal-distribution-ex3.gif`/>

Or perhaps we could have some combination of better accuracy and slightly larger average size, I will leave that up to you!


CORRELATION
A correlation is a statistical measure of the relationship between two variables. The measure is best used in variables that demonstrate a linear relationship between each other. The fit of the data can be visually represented in a scatterplot.

The correlation COEFFICIENT is a value that indicates the strength of the relationship. The coefficient can take any values from -1 to 1. The interpretations of the values are:
	-1: Perfect negative correlation. The variables tend to move in opposite directions (i.e., when one variable increases, the other variable decreases).
	0 : No correlation. The variables do not have a relationship with each other.
	1 : Perfect positive correlation. The variables tend to move in the same direction (i.e., when one variable increases, the other variable also increases).

If two variables are correlated, it does not imply that one variable causes the changes in another variable. Correlation only assesses relationships between variables, and there may be different factors that lead to the relationships. CAUSATION may be a reason for the correlation, but it is not the only possible explanation.
CORRELATION FORMULAE
  	 __________________________________________  
    /         	Σ (xᵢ - x̅) (yᵢ - ȳ)			  /
   / r(xy) = ______________________________  / 
  /	          _________________________     /
 / 			 √(Σ (xᵢ - x̅)² Σ(yᵢ - ȳ)²)     / 
/_________________________________________/
	rxy – the correlation coefficient of the linear relationship between the variables x and y
	xᵢ – the values of the x-variable in a sample
	x̅ – the mean of the values of the x-variable
	yᵢ – the values of the y-variable in a sample
	ȳ – the mean of the values of the y-variable



COVARIANCE
Covariance is a measure of the relationship between two random variables. The metric evaluates how much – to what extent – the variables change together. In other words, it is essentially a measure of the variance between two variables (note that the variance of one variable equals the variance of the other variable). However, the metric does not assess the dependency between variables.
Unlike the correlation coefficient, covariance is measured in units. The units are computed by multiplyᵢng the units of the two variables. The variance can take any positive or negative values. The values are interpreted as follows:
	Positive covariance: Indicates that two variables tend to move in the same direction.
	Negative covariance: Reveals that two variables tend to move in inverse directions.

 	__________________________________________    __________________________________________
   /         		Σ (xᵢ - x̅) (yᵢ - ȳ)		 /   /         		Σ (xᵢ - x̅) (yᵢ - ȳ)		   /
  / Cov(x,y) = ___________________________  /   / Cov(x,y) = ___________________________  /   
 /	        			n 				   /   /	        			n-1			     / 
/_________________________________________/   /_________________________________________/
		Population COVARIANCE 								Sample COVARIANCE


COVARIANCE VS. CORRELATION
Covariance and correlation both primarily assess the relationship between variables. The closest analogy to the relationship between them is the relationship between the variance and standard deviation.

Covariance measures the total variation of two random variables from their expected values. Using covariance, we can only gauge the direction of the relationship (whether the variables tend to move in tandem or show an inverse relationship). However, it does not indicate the strength of the relationship, nor the dependency between the variables.

On the other hand, correlation measures the strength of the relationship between variables. Correlation is the scaled measure of covariance. It is dimensionless. In other words, the correlation coefficient is always a pure value and not measured in any units.

The relationship between the two concepts can be expressed using the formula below:
		_____________________________
	   /	  		Cov(x,y)		/
	  /	p(X,Y) = __________        / 
	 /				σx σy         /
	/____________________________/
****************MEAN, MEDIAN, MODE, GEOMETRIC MEAN, HARMONIC MEAN, VARIANCE, COVARIANCE, CORELATION ¹²³⁴⁵⁶⁷⁸⁹⁰****************


####################FOOTER################################
https://github.com/llSourcell/LoanDefault-Prediction
https://www.lendingclub.com/info/download-data.action
####################FOOTER################################